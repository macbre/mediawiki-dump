"""
Cleans and tokenizes given text
"""

import re
from typing import Callable, List


def clean(text: str) -> str:
    """Cleans up the provided wikitext.
    Removes templates, tables, parser hooks, magic words, HTML tags and file embeds.
    Keeps links.
    """
    # basic formatting
    text = re.sub(r"'''?([^']+)'''?", "\\1", text)

    # templates
    # {{foo}}
    # {{foo|{{test}}|123}}
    while "{{" in text:
        start = text.find("{{")
        level = 1
        pos = start + 2

        while pos < len(text):
            # print('>' + text[pos:pos+2] + '<')

            if text[pos : pos + 2] == "{{":
                # nested template - enter next level
                level += 1
                pos += 1
            elif text[pos : pos + 2] == "}}":
                # nested template - leave this level
                pos += 1
                level -= 1

            # template is now completed
            if level == 0:
                # print(text, start, pos, text[start:pos+1])
                text = text[:start] + " " + text[pos + 1 :]
                break

            # check next character
            pos += 1

        # the template is not well balanced, leave the endless loop
        if level != 0:
            break

    # tables
    text = re.sub(r"{\|[^}]+\|}", "", text)  # {|foo..|}

    # headings
    text = re.sub(
        r"^=+\s?([^=]+)\s?=+",
        lambda matches: matches.group(1).strip(),
        text,
        flags=re.MULTILINE,
    )  # == a == -> a

    # files and other links with namespaces
    text = re.sub(r"\[\[[^:\]]+:[^\]]+\]\]", "", text)  # [[foo:b]] -> ''

    # local links
    text = re.sub(r"\[\[([^|\]]+)\]\]", "\\1", text)  # [[a]] -> a
    text = re.sub(r"\[\[[^|]+\|([^\]]+)\]\]", "\\1", text)  # [[a|b]] -> b

    text = text.replace("[[", "").replace("]]", "")

    # external links
    text = re.sub(
        r"\[http[^\s]+ ([^\]]+)\]", "\\1", text
    )  # [[http://example.com foo]] -> foo
    text = re.sub(r"https?://[^\s]+", "", text)  # remove http://example.com

    # lists
    text = re.sub(r"^\*+\s?", "", text, flags=re.MULTILINE)

    # parser hooks
    text = re.sub(r"<[^>]+>[^<]+</[^>]+>", " ", text)  # <ref>foo</ref>

    # HTML
    text = re.sub(r"<[^>]+/?>", " ", text)  # <br> / <br />
    text = text.replace("&nbsp;", " ")

    # magic words
    text = re.sub(r"__\w+__", "", text)  # __TOC__

    return text.strip()


def tokenize_filter(text: str) -> bool:
    """Used by tokenize function below. Ignores numbers when tokenizing cleaned up wikitext."""
    if text == "":
        return False

    if text[0].isdigit() and re.fullmatch(r"\d+", text):
        return False

    return True


def tokenize(text: str, filter_func: Callable = tokenize_filter) -> List[str]:
    """Tokenizes a given text. Usually you want to first pass it via clean() function."""
    # clean up the text
    text = re.sub(r'[?.,:;!()=+"]', " ", text)  # remove noise

    text = text.strip()

    # tokenize
    parts = re.split(r"[-â€“\s/|_&{}\xAD]", text)

    parts = filter(filter_func, parts)

    return list(parts)
